[toc]
# 容器与云原生八股文

## 一、Docker 基础

### 1. Docker 核心概念

**镜像（Image）**：只读模板，包含运行应用所需的一切
**容器（Container）**：镜像的运行实例
**仓库（Repository）**：存储和分发镜像的地方

**关系图：**
```
仓库（Docker Hub）
    ↓ pull
镜像（Image）
    ↓ run
容器（Container）
    ↓ commit
新镜像
```

**镜像特点：**
- 分层存储（Union FS）
- 只读层 + 可写层
- 共享基础层，节省空间

### 2. Docker 架构

**Client-Server 架构：**
- Docker Client：用户交互界面
- Docker Daemon：后台服务进程
- Docker Registry：镜像仓库

**核心组件：**
- containerd：容器运行时
- runc：OCI 运行时实现
- shim：容器进程管理

### 3. Docker 网络模式

**bridge（默认）**：
- 容器有独立IP
- 通过NAT访问外网
- 容器间可通过IP通信

**host**：
- 容器使用宿主机网络
- 性能最好
- 端口冲突风险

**none**：
- 容器无网络
- 完全隔离

**container**：
- 共享其他容器的网络
- 用于sidecar模式

**自定义网络**：
```bash
docker network create my-network
docker run -d --network my-network --name web nginx
```

### 4. Docker 存储

**存储驱动：**
- overlay2（推荐）：性能好，支持多层
- aufs：旧版，已淘汰
- devicemapper：CentOS默认
- btrfs：支持快照
- zfs：功能强大，资源消耗大

**数据持久化：**

| 类型 | 管理 | 性能 | 适用场景 |
|------|------|------|----------|
| Volume | Docker管理 | 好 | 数据持久化 |
| Bind Mount | 用户管理 | 好 | 开发调试 |
| tmpfs | 内存 | 最好 | 临时数据 |

### 5. Dockerfile 最佳实践

**多阶段构建：**
```dockerfile
FROM golang:1.21 AS builder
WORKDIR /app
COPY . .
RUN go build -o main .

FROM alpine:latest
COPY --from=builder /app/main .
CMD ["./main"]
```

**优化技巧：**
- 使用Alpine基础镜像
- 合并RUN命令减少层数
- 利用构建缓存
- 使用.dockerignore
- 清理缓存文件

### 6. Docker Compose

**作用：**
- 定义多容器应用
- 一键启动所有服务
- 管理服务依赖

**示例：**
```yaml
version: '3.8'
services:
  web:
    build: .
    ports:
      - "8080:80"
    depends_on:
      - db
  db:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: password
    volumes:
      - db-data:/var/lib/mysql
volumes:
  db-data:
```

### 7. Docker 安全

**安全措施：**
- 使用非root用户运行
- 限制容器资源
- 使用只读文件系统
- 扫描镜像漏洞
- 使用私有仓库
- 启用内容信任

**资源限制：**
```bash
docker run -m 512m --cpus=1 nginx
```

### 8. Docker 命令详解

**镜像操作：**
```bash
# 拉取镜像
docker pull nginx:latest

# 查看镜像
docker images

# 删除镜像
docker rmi nginx:latest

# 构建镜像
docker build -t myapp:v1 .

# 推送镜像
docker push myapp:v1

# 导出镜像
docker save -o nginx.tar nginx:latest

# 导入镜像
docker load -i nginx.tar

# 查看镜像历史
docker history nginx:latest

# 查看镜像详情
docker inspect nginx:latest
```

**容器操作：**
```bash
# 运行容器
docker run -d --name web -p 80:80 nginx

# 查看运行中的容器
docker ps

# 查看所有容器
docker ps -a

# 停止容器
docker stop web

# 启动容器
docker start web

# 重启容器
docker restart web

# 删除容器
docker rm web

# 进入容器
docker exec -it web bash

# 查看容器日志
docker logs -f web

# 查看容器资源使用
docker stats web

# 复制文件
docker cp file.txt web:/app/

# 提交容器为镜像
docker commit web myapp:v1
```

### 9. Docker 网络详解

**网络命令：**
```bash
# 创建网络
docker network create --driver bridge my-network

# 查看网络列表
docker network ls

# 查看网络详情
docker network inspect my-network

# 连接容器到网络
docker network connect my-network container_id

# 断开网络连接
docker network disconnect my-network container_id

# 删除网络
docker network rm my-network
```

**跨主机网络：**
- Overlay网络：Swarm模式
- Macvlan：直接分配MAC地址
- Host：使用宿主机网络栈

### 10. Docker Volume 详解

**Volume 命令：**
```bash
# 创建volume
docker volume create my-data

# 查看volume列表
docker volume ls

# 查看volume详情
docker volume inspect my-data

# 删除volume
docker volume rm my-data

# 清理未使用的volume
docker volume prune
```

**Volume 驱动：**
- local：本地存储
- nfs：网络文件系统
- cifs：Windows共享
- 第三方驱动：AWS EBS、Azure Disk等


## 二、Kubernetes 核心

### 1. K8s 架构

**控制平面组件：**

**kube-apiserver**：
- API网关，所有操作的入口
- 认证、授权、准入控制
- 与etcd交互
- RESTful API
- 水平扩展

**etcd**：
- 分布式KV存储
- 存储集群所有数据
- 使用Raft协议保证一致性
- 支持watch机制
- 强一致性

**kube-scheduler**：
- 调度Pod到合适的Node
- 考虑资源、亲和性、污点等
- 预选和优选算法
- 支持自定义调度器

**kube-controller-manager**：
- 运行各种控制器
- Node控制器、Deployment控制器等
- 维护集群期望状态
- 控制循环模式

**cloud-controller-manager**：
- 与云平台交互
- 管理负载均衡、存储等
- 节点生命周期管理

**Node组件：**

**kubelet**：
- Node代理
- 管理Pod生命周期
- 上报Node状态
- 执行健康检查
- 资源监控

**kube-proxy**：
- 网络代理
- 实现Service负载均衡
- 维护iptables/ipvs规则
- 支持多种代理模式

**容器运行时**：
- Docker、containerd、CRI-O
- 实现CRI接口
- 管理容器生命周期

### 2. Pod 详解

**Pod 生命周期：**
1. Pending：已创建，等待调度
2. Running：已调度，容器运行中
3. Succeeded：所有容器成功终止
4. Failed：至少一个容器失败
5. Unknown：无法获取状态

**Pod 阶段转换：**
```
创建 → Pending → Running → Succeeded/Failed
                    ↓
                 Unknown
```

**探针类型：**

**livenessProbe（存活探针）**：
- 检测容器是否存活
- 失败则重启容器
- 用于检测死锁

**readinessProbe（就绪探针）**：
- 检测容器是否就绪
- 失败则移出Service端点
- 用于流量控制

**startupProbe（启动探针）**：
- 检测容器是否启动完成
- 慢启动应用
- 避免过早检测

**探测方式：**
```yaml
# HTTP GET
livenessProbe:
  httpGet:
    path: /health
    port: 8080
    httpHeaders:
    - name: Custom-Header
      value: Awesome
  initialDelaySeconds: 3
  periodSeconds: 3
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 3

# TCP Socket
livenessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 15
  periodSeconds: 20

# Exec Command
livenessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
```

**Init 容器：**
- 在主容器前运行
- 顺序执行
- 必须成功完成
- 用于初始化工作

**示例：**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do sleep 2; done']
  - name: init-mydb
    image: busybox
    command: ['sh', '-c', 'until nslookup mydb; do sleep 2; done']
  containers:
  - name: myapp
    image: myapp:latest
```

**生命周期钩子：**

**postStart**：
- 容器启动后立即执行
- 异步执行
- 失败会重启容器

**preStop**：
- 容器终止前执行
- 同步执行
- 用于优雅关闭

```yaml
lifecycle:
  postStart:
    exec:
      command: ["/bin/sh", "-c", "echo Hello from postStart"]
  preStop:
    exec:
      command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
```

### 3. 工作负载控制器

**Deployment**：
- 无状态应用
- 滚动更新
- 版本回滚
- 水平扩展
- 声明式更新

**特性：**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  revisionHistoryLimit: 10  # 保留历史版本数
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

**StatefulSet**：
- 有状态应用
- 稳定的网络标识
- 有序部署和扩展
- 持久化存储
- 有序滚动更新

**特性：**
- Pod名称：{StatefulSet名称}-{序号}
- 稳定的hostname
- 有序创建：0, 1, 2...
- 有序删除：...2, 1, 0
- 独立的PVC

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast
      resources:
        requests:
          storage: 10Gi
```

**DaemonSet**：
- 每个Node一个Pod
- 日志收集、监控
- 自动调度到新Node
- 节点级别服务

**使用场景：**
- 日志收集：Fluentd、Logstash
- 监控：Node Exporter、cAdvisor
- 网络：Calico、Flannel
- 存储：Ceph、GlusterFS

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluentd:latest
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

**Job**：
- 一次性任务
- 保证完成
- 支持并行
- 失败重试

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  completions: 5        # 成功完成5次
  parallelism: 2        # 并行2个Pod
  backoffLimit: 4       # 失败重试4次
  activeDeadlineSeconds: 100  # 超时时间
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

**CronJob**：
- 定时任务
- Cron表达式
- 并发策略
- 历史限制

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"  # 每分钟执行
  concurrencyPolicy: Forbid  # 禁止并发
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["/bin/sh", "-c", "date; echo Hello"]
          restartPolicy: OnFailure
```

### 4. Service 和服务发现

**Service 类型：**

**ClusterIP（默认）**：
- 集群内部访问
- 虚拟IP（VIP）
- kube-proxy负载均衡

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  sessionAffinity: ClientIP  # 会话保持
```

**NodePort**：
- 通过Node IP:NodePort访问
- 端口范围：30000-32767
- 自动创建ClusterIP

```yaml
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080
```

**LoadBalancer**：
- 云平台负载均衡器
- 自动分配外部IP
- 自动创建NodePort和ClusterIP

```yaml
spec:
  type: LoadBalancer
  loadBalancerIP: 1.2.3.4
  ports:
  - port: 80
    targetPort: 8080
```

**ExternalName**：
- DNS CNAME记录
- 访问外部服务
- 无代理

```yaml
spec:
  type: ExternalName
  externalName: api.example.com
```

**Headless Service**：
- ClusterIP设为None
- 直接返回Pod IP
- 用于StatefulSet
- 服务发现

```yaml
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - port: 3306
```

**服务发现：**

**环境变量**：
```bash
MYSQL_SERVICE_HOST=10.0.0.11
MYSQL_SERVICE_PORT=3306
```

**DNS**：
```
# Service
<service-name>.<namespace>.svc.cluster.local

# Pod
<pod-ip>.<namespace>.pod.cluster.local

# StatefulSet Pod
<pod-name>.<service-name>.<namespace>.svc.cluster.local
```

**Endpoints**：
```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
- addresses:
  - ip: 192.168.1.1
  - ip: 192.168.1.2
  ports:
  - port: 9376
```

### 5. Ingress 详解

**作用：**
- HTTP/HTTPS路由
- 域名访问
- SSL终止
- 负载均衡
- 虚拟主机

**Ingress Controller：**
- Nginx Ingress
- Traefik
- HAProxy
- Kong
- Istio Gateway

**基本示例：**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - example.com
    secretName: tls-secret
  rules:
  - host: example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
      - path: /web
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

**路径类型：**
- Exact：精确匹配
- Prefix：前缀匹配
- ImplementationSpecific：实现特定

**常用注解：**
```yaml
annotations:
  # 重写目标
  nginx.ingress.kubernetes.io/rewrite-target: /$2
  
  # 限流
  nginx.ingress.kubernetes.io/limit-rps: "10"
  
  # 超时
  nginx.ingress.kubernetes.io/proxy-connect-timeout: "30"
  nginx.ingress.kubernetes.io/proxy-send-timeout: "30"
  nginx.ingress.kubernetes.io/proxy-read-timeout: "30"
  
  # 白名单
  nginx.ingress.kubernetes.io/whitelist-source-range: "10.0.0.0/8"
  
  # 认证
  nginx.ingress.kubernetes.io/auth-type: basic
  nginx.ingress.kubernetes.io/auth-secret: basic-auth
  
  # CORS
  nginx.ingress.kubernetes.io/enable-cors: "true"
  
  # 会话保持
  nginx.ingress.kubernetes.io/affinity: "cookie"
```

### 6. 配置管理

**ConfigMap**：
- 存储配置数据
- 明文存储
- 环境变量或文件挂载
- 热更新（Volume挂载）

**创建方式：**
```bash
# 从文件创建
kubectl create configmap app-config --from-file=config.yaml

# 从目录创建
kubectl create configmap app-config --from-file=config/

# 从字面值创建
kubectl create configmap app-config --from-literal=key1=value1 --from-literal=key2=value2
```

**YAML定义：**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # 键值对
  LOG_LEVEL: "info"
  DATABASE_URL: "mysql://localhost:3306"
  
  # 文件内容
  app.conf: |
    server {
      listen 80;
      server_name example.com;
    }
  
  config.json: |
    {
      "port": 8080,
      "debug": true
    }
```

**使用方式：**
```yaml
spec:
  containers:
  - name: app
    # 环境变量 - 单个key
    env:
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: LOG_LEVEL
    
    # 环境变量 - 所有key
    envFrom:
    - configMapRef:
        name: app-config
    
    # Volume挂载
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
  
  volumes:
  - name: config
    configMap:
      name: app-config
      items:
      - key: app.conf
        path: nginx.conf
```

**Secret**：
- 存储敏感数据
- Base64编码
- 支持多种类型
- 加密存储（etcd）

**Secret 类型：**
- Opaque：通用Secret
- kubernetes.io/tls：TLS证书
- kubernetes.io/dockerconfigjson：Docker认证
- kubernetes.io/basic-auth：基本认证
- kubernetes.io/ssh-auth：SSH认证
- kubernetes.io/service-account-token：SA令牌

**创建方式：**
```bash
# 从文件创建
kubectl create secret generic db-secret --from-file=username.txt --from-file=password.txt

# 从字面值创建
kubectl create secret generic db-secret --from-literal=username=admin --from-literal=password=secret

# TLS Secret
kubectl create secret tls tls-secret --cert=tls.crt --key=tls.key

# Docker Registry Secret
kubectl create secret docker-registry regcred \
  --docker-server=registry.example.com \
  --docker-username=user \
  --docker-password=pass \
  --docker-email=user@example.com
```

**YAML定义：**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: YWRtaW4=  # base64编码
  password: cGFzc3dvcmQ=
stringData:
  config.yaml: |  # 自动编码
    username: admin
    password: password
```

**使用方式：**
```yaml
spec:
  containers:
  - name: app
    # 环境变量
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    
    # Volume挂载
    volumeMounts:
    - name: secret
      mountPath: /etc/secret
      readOnly: true
  
  # ImagePullSecrets
  imagePullSecrets:
  - name: regcred
  
  volumes:
  - name: secret
    secret:
      secretName: db-secret
      defaultMode: 0400
```


### 7. 存储详解

**Volume 类型：**

**emptyDir**：
- 临时存储
- Pod删除时清空
- 可选内存存储

```yaml
volumes:
- name: cache
  emptyDir:
    medium: Memory  # 使用内存
    sizeLimit: 1Gi
```

**hostPath**：
- 宿主机路径
- 节点本地存储
- 不推荐生产使用

```yaml
volumes:
- name: data
  hostPath:
    path: /data
    type: DirectoryOrCreate
```

**nfs**：
- 网络文件系统
- 多节点共享
- ReadWriteMany

```yaml
volumes:
- name: nfs
  nfs:
    server: nfs-server.example.com
    path: /exported/path
```

**persistentVolumeClaim**：
- 持久化存储
- 动态供应
- 生命周期独立

**PV 和 PVC：**

**PersistentVolume（PV）**：
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    server: nfs-server.example.com
    path: /exported/path
```

**PersistentVolumeClaim（PVC）**：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: nfs
  resources:
    requests:
      storage: 5Gi
```

**访问模式：**
- ReadWriteOnce（RWO）：单节点读写
- ReadOnlyMany（ROX）：多节点只读
- ReadWriteMany（RWX）：多节点读写
- ReadWriteOncePod（RWOP）：单Pod读写

**回收策略：**
- Retain：保留（手动回收）
- Delete：删除
- Recycle：回收（已废弃）

**StorageClass**：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

**动态供应：**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 10Gi
```

**卷扩展：**
```bash
# 修改PVC大小
kubectl edit pvc dynamic-pvc
# 修改 storage: 20Gi

# 查看状态
kubectl get pvc dynamic-pvc
```

**快照：**
```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: snapshot-1
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: pvc-nfs
```

### 8. 调度详解

**调度流程：**
1. **预选（Predicate）**：过滤不符合条件的Node
   - PodFitsResources：资源充足
   - PodFitsHostPorts：端口不冲突
   - HostName：指定主机名
   - MatchNodeSelector：匹配节点选择器
   - NoVolumeZoneConflict：卷区域冲突

2. **优选（Priority）**：给Node打分
   - LeastRequestedPriority：资源使用率低优先
   - BalancedResourceAllocation：资源均衡
   - SelectorSpreadPriority：Pod分散
   - NodeAffinityPriority：节点亲和性

3. **选定（Select）**：选择分数最高的Node

**调度策略：**

**nodeSelector**：
```yaml
spec:
  nodeSelector:
    disktype: ssd
    zone: us-west-1a
```

**节点亲和性（Node Affinity）**：
```yaml
spec:
  affinity:
    nodeAffinity:
      # 硬性要求
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/zone
            operator: In
            values:
            - us-west-1a
            - us-west-1b
      # 软性要求
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**Pod 亲和性（Pod Affinity）**：
```yaml
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: cache
        topologyKey: kubernetes.io/hostname
```

**Pod 反亲和性（Pod Anti-Affinity）**：
```yaml
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: web
          topologyKey: kubernetes.io/hostname
```

**污点和容忍（Taint & Toleration）**：

**添加污点：**
```bash
# NoSchedule：不调度新Pod
kubectl taint nodes node1 key=value:NoSchedule

# PreferNoSchedule：尽量不调度
kubectl taint nodes node1 key=value:PreferNoSchedule

# NoExecute：驱逐已有Pod
kubectl taint nodes node1 key=value:NoExecute

# 删除污点
kubectl taint nodes node1 key:NoSchedule-
```

**容忍：**
```yaml
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"
  
  - key: "key"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600  # 容忍3600秒
```

**优先级（Priority）**：
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "High priority class"
---
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-pod
spec:
  priorityClassName: high-priority
  containers:
  - name: nginx
    image: nginx
```

**拓扑分布约束：**
```yaml
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: web
```

### 9. 自动扩缩容

**HPA（水平扩缩容）**：
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # CPU指标
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  # 内存指标
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # 自定义指标
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  
  # 外部指标
  - type: External
    external:
      metric:
        name: queue_messages_ready
        selector:
          matchLabels:
            queue: worker_tasks
      target:
        type: AverageValue
        averageValue: "30"
  
  # 扩缩容行为
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
```

**VPA（垂直扩缩容）**：
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Recreate, Auto
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
```

**Cluster Autoscaler**：
- 自动增删节点
- 基于Pod调度需求
- 云平台集成

**KEDA（事件驱动自动扩缩容）**：
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rabbitmq-scaledobject
spec:
  scaleTargetRef:
    name: rabbitmq-consumer
  minReplicaCount: 0
  maxReplicaCount: 30
  triggers:
  - type: rabbitmq
    metadata:
      queueName: myqueue
      queueLength: "5"
```

### 10. 更新策略

**滚动更新（Rolling Update）**：
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2        # 最多超出2个Pod
      maxUnavailable: 1  # 最多1个Pod不可用
  template:
    spec:
      containers:
      - name: app
        image: app:v2
```

**更新命令：**
```bash
# 更新镜像
kubectl set image deployment/app app=app:v2 --record

# 查看更新状态
kubectl rollout status deployment/app

# 查看历史
kubectl rollout history deployment/app

# 查看指定版本详情
kubectl rollout history deployment/app --revision=2

# 回滚到上一版本
kubectl rollout undo deployment/app

# 回滚到指定版本
kubectl rollout undo deployment/app --to-revision=2

# 暂停更新
kubectl rollout pause deployment/app

# 恢复更新
kubectl rollout resume deployment/app

# 重启
kubectl rollout restart deployment/app
```

**蓝绿部署：**
```yaml
# 蓝色版本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: app:v1

---
# 绿色版本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: app:v2

---
# Service切换
apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  selector:
    app: myapp
    version: blue  # 切换到green
  ports:
  - port: 80
```

**金丝雀发布：**
```yaml
# 稳定版本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-stable
spec:
  replicas: 9
  template:
    metadata:
      labels:
        app: myapp
        track: stable
    spec:
      containers:
      - name: app
        image: app:v1

---
# 金丝雀版本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-canary
spec:
  replicas: 1  # 10%流量
  template:
    metadata:
      labels:
        app: myapp
        track: canary
    spec:
      containers:
      - name: app
        image: app:v2

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  selector:
    app: myapp  # 同时选择stable和canary
  ports:
  - port: 80
```

### 11. 资源管理

**资源请求和限制：**
```yaml
spec:
  containers:
  - name: app
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
        ephemeral-storage: "2Gi"
      limits:
        memory: "128Mi"
        cpu: "500m"
        ephemeral-storage: "4Gi"
```

**QoS 类别：**

**Guaranteed（最高优先级）**：
- requests = limits
- 所有容器都设置
- 最后被驱逐

**Burstable（中等优先级）**：
- requests < limits
- 至少一个容器设置requests

**BestEffort（最低优先级）**：
- 无requests和limits
- 最先被驱逐

**ResourceQuota（命名空间配额）**：
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    # 计算资源
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    
    # 对象数量
    pods: "50"
    services: "10"
    services.loadbalancers: "2"
    services.nodeports: "5"
    persistentvolumeclaims: "20"
    configmaps: "10"
    secrets: "10"
    
    # 存储
    requests.storage: "100Gi"
    persistentvolumeclaims: "10"
```

**LimitRange（默认资源限制）**：
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: dev
spec:
  limits:
  # Container级别
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "2"
  
  # Pod级别
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"
  
  # PVC级别
  - type: PersistentVolumeClaim
    max:
      storage: "10Gi"
    min:
      storage: "1Gi"
```

**查看资源使用：**
```bash
# 查看Node资源
kubectl top nodes

# 查看Pod资源
kubectl top pods

# 查看Pod资源（所有命名空间）
kubectl top pods --all-namespaces

# 查看容器资源
kubectl top pods --containers

# 查看配额
kubectl describe quota -n dev

# 查看LimitRange
kubectl describe limitrange -n dev
```

### 12. RBAC 权限控制

**核心概念：**
- Role：命名空间级别角色
- ClusterRole：集群级别角色
- RoleBinding：绑定Role到用户/组/SA
- ClusterRoleBinding：绑定ClusterRole

**Role 示例：**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

**ClusterRole 示例：**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
- nonResourceURLs: ["*"]
  verbs: ["*"]
```

**RoleBinding 示例：**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# 用户
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io

# 组
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io

# ServiceAccount
- kind: ServiceAccount
  name: default
  namespace: default

roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

**ClusterRoleBinding 示例：**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-binding
subjects:
- kind: User
  name: admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

**ServiceAccount：**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
  namespace: default
---
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: my-sa
  containers:
  - name: app
    image: myapp
```

**常用命令：**
```bash
# 检查权限
kubectl auth can-i create deployments --namespace=dev

# 以其他用户身份检查
kubectl auth can-i list pods --as=jane --namespace=dev

# 查看角色
kubectl get roles,clusterroles

# 查看绑定
kubectl get rolebindings,clusterrolebindings

# 查看ServiceAccount
kubectl get sa

# 创建ServiceAccount
kubectl create sa my-sa
```

**预定义角色：**
- cluster-admin：超级管理员
- admin：命名空间管理员
- edit：编辑权限
- view：只读权限


### 13. 网络策略

**NetworkPolicy**：
- 控制Pod间通信
- 入站和出站规则
- 基于标签选择
- 需要CNI支持（Calico、Cilium等）

**默认拒绝所有入站流量：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

**默认拒绝所有出站流量：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress
```

**允许特定Pod访问：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

**允许特定命名空间访问：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-namespace
spec:
  podSelector:
    matchLabels:
      app: api
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          env: prod
    ports:
    - protocol: TCP
      port: 80
```

**允许外部IP访问：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - from:
    - ipBlock:
        cidr: 10.0.0.0/8
        except:
        - 10.0.1.0/24
    ports:
    - protocol: TCP
      port: 80
```

**出站规则：**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-egress
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
  - Egress
  egress:
  # 允许DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问外部API
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
    ports:
    - protocol: TCP
      port: 443
```

### 14. 健康检查和监控

**探针配置详解：**
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
    httpHeaders:
    - name: X-Custom-Header
      value: Awesome
  initialDelaySeconds: 30  # 初始延迟
  periodSeconds: 10        # 检查间隔
  timeoutSeconds: 5        # 超时时间
  successThreshold: 1      # 成功阈值
  failureThreshold: 3      # 失败阈值

readinessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10

startupProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 0
  periodSeconds: 10
  failureThreshold: 30  # 最多等待300秒
```

**Metrics Server**：
```bash
# 安装
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 查看资源使用
kubectl top nodes
kubectl top pods
```

**Prometheus 监控：**
```yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: app-monitor
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

**自定义指标：**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  labels:
    app: myapp
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  ports:
  - name: metrics
    port: 8080
  selector:
    app: myapp
```

## 三、容器运行时

### 1. CRI（容器运行时接口）

**作用：**
- 标准化容器运行时接口
- 解耦K8s和容器运行时
- 支持多种运行时
- 插件化架构

**CRI 接口：**
- RuntimeService：容器和Pod生命周期管理
- ImageService：镜像管理

**实现：**
- containerd：CNCF项目，轻量级
- CRI-O：专为K8s设计
- Docker：通过dockershim（已废弃）

### 2. containerd

**特点：**
- 轻量级
- 高性能
- OCI兼容
- K8s官方推荐
- 独立于Docker

**架构：**
```
kubelet
  ↓ CRI
containerd
  ↓
containerd-shim
  ↓
runc (OCI Runtime)
  ↓
Container
```

**配置：**
```toml
# /etc/containerd/config.toml
version = 2

[plugins."io.containerd.grpc.v1.cri"]
  [plugins."io.containerd.grpc.v1.cri".containerd]
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      runtime_type = "io.containerd.runc.v2"
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true
```

**命令：**
```bash
# 查看容器
ctr containers ls

# 查看镜像
ctr images ls

# 拉取镜像
ctr images pull docker.io/library/nginx:latest

# 运行容器
ctr run docker.io/library/nginx:latest nginx

# 使用crictl（K8s专用）
crictl ps
crictl images
crictl logs <container-id>
crictl exec -it <container-id> sh
```

### 3. OCI 规范

**Runtime Spec（运行时规范）**：
- 容器配置格式
- 生命周期管理
- 运行时操作
- config.json

**Image Spec（镜像规范）**：
- 镜像格式
- 镜像清单（Manifest）
- 镜像配置
- 文件系统层

**Distribution Spec（分发规范）**：
- 镜像仓库API
- 镜像推送和拉取
- 内容寻址

**OCI Runtime（runc）**：
```bash
# 创建容器
runc create mycontainer

# 启动容器
runc start mycontainer

# 查看容器
runc list

# 删除容器
runc delete mycontainer
```

### 4. 容器隔离技术

**Namespace（命名空间）**：

**PID Namespace**：
- 进程隔离
- 容器内PID从1开始
- 看不到宿主机进程

**NET Namespace**：
- 网络隔离
- 独立的网络栈
- 独立的IP、路由、端口

**IPC Namespace**：
- 进程间通信隔离
- 独立的消息队列
- 独立的信号量

**MNT Namespace**：
- 文件系统隔离
- 独立的挂载点
- 根文件系统

**UTS Namespace**：
- 主机名隔离
- 独立的hostname
- 独立的domainname

**USER Namespace**：
- 用户隔离
- UID/GID映射
- 容器内root非宿主机root

**Cgroups（控制组）**：

**CPU 限制：**
```bash
# CPU份额
echo 512 > /sys/fs/cgroup/cpu/mycontainer/cpu.shares

# CPU周期
echo 100000 > /sys/fs/cgroup/cpu/mycontainer/cpu.cfs_period_us
echo 50000 > /sys/fs/cgroup/cpu/mycontainer/cpu.cfs_quota_us
```

**内存限制：**
```bash
# 内存限制
echo 536870912 > /sys/fs/cgroup/memory/mycontainer/memory.limit_in_bytes

# OOM控制
echo 1 > /sys/fs/cgroup/memory/mycontainer/memory.oom_control
```

**IO 限制：**
```bash
# 读写速率
echo "8:0 1048576" > /sys/fs/cgroup/blkio/mycontainer/blkio.throttle.read_bps_device
echo "8:0 1048576" > /sys/fs/cgroup/blkio/mycontainer/blkio.throttle.write_bps_device
```

**网络限制：**
- tc（Traffic Control）
- 带宽限制
- 流量整形

**Union FS（联合文件系统）**：

**OverlayFS**：
- lowerdir：只读层（镜像层）
- upperdir：可写层（容器层）
- merged：合并视图
- workdir：工作目录

**特点：**
- 写时复制（CoW）
- 节省空间
- 快速启动
- 层共享

## 四、服务网格（Service Mesh）

### 1. Istio 架构

**控制平面（Istiod）**：
- Pilot：服务发现和流量管理
- Citadel：证书管理和身份认证
- Galley：配置管理和验证

**数据平面：**
- Envoy Sidecar：代理所有流量
- 透明拦截
- 七层代理

**核心功能：**
- 流量管理
- 安全
- 可观测性
- 策略执行

### 2. 流量管理

**VirtualService（虚拟服务）**：
```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  # 金丝雀发布
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  
  # 流量分割
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 90
    - destination:
        host: reviews
        subset: v2
      weight: 10
  
  # 超时
  timeout: 10s
  
  # 重试
  retries:
    attempts: 3
    perTryTimeout: 2s
    retryOn: 5xx,reset,connect-failure
```

**DestinationRule（目标规则）**：
```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  trafficPolicy:
    # 负载均衡
    loadBalancer:
      simple: LEAST_REQUEST
    
    # 连接池
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 10
        http2MaxRequests: 100
        maxRequestsPerConnection: 2
    
    # 熔断器
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 40
  
  # 子集
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
```

**Gateway（网关）**：
```yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: my-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "example.com"
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: example-credential
    hosts:
    - "example.com"
```

### 3. 安全

**mTLS（双向TLS）**：
```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: default
spec:
  mtls:
    mode: STRICT  # PERMISSIVE, STRICT, DISABLE
```

**授权策略：**
```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-read
spec:
  selector:
    matchLabels:
      app: httpbin
  action: ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/sleep"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/info*"]
    when:
    - key: request.headers[version]
      values: ["v1", "v2"]
```

**请求认证：**
```yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
spec:
  selector:
    matchLabels:
      app: httpbin
  jwtRules:
  - issuer: "testing@secure.istio.io"
    jwksUri: "https://raw.githubusercontent.com/istio/istio/release-1.16/security/tools/jwt/samples/jwks.json"
```

### 4. 可观测性

**指标（Metrics）**：
- 请求量（RPS）
- 延迟（P50、P95、P99）
- 错误率
- 流量分布

**追踪（Tracing）**：
```yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: mesh-default
spec:
  tracing:
  - providers:
    - name: jaeger
    randomSamplingPercentage: 100.0
```

**日志（Logging）**：
```yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: access-log
spec:
  accessLogging:
  - providers:
    - name: envoy
    filter:
      expression: response.code >= 400
```

### 5. 流量控制

**熔断（Circuit Breaking）**：
```yaml
trafficPolicy:
  outlierDetection:
    consecutiveErrors: 5
    interval: 30s
    baseEjectionTime: 30s
    maxEjectionPercent: 50
```

**限流（Rate Limiting）**：
```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: filter-ratelimit
spec:
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.local_ratelimit
        typed_config:
          "@type": type.googleapis.com/udpa.type.v1.TypedStruct
          type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
          value:
            stat_prefix: http_local_rate_limiter
            token_bucket:
              max_tokens: 100
              tokens_per_fill: 100
              fill_interval: 60s
```

**重试（Retry）**：
```yaml
retries:
  attempts: 3
  perTryTimeout: 2s
  retryOn: 5xx,reset,connect-failure,refused-stream
```

**超时（Timeout）**：
```yaml
timeout: 10s
```

**故障注入（Fault Injection）**：
```yaml
fault:
  delay:
    percentage:
      value: 10
    fixedDelay: 5s
  abort:
    percentage:
      value: 10
    httpStatus: 500
```

## 五、云原生存储

### 1. CSI（容器存储接口）

**作用：**
- 标准化存储接口
- 插件化架构
- 支持多种存储
- 动态供应

**组件：**

**CSI Controller**：
- CreateVolume：创建卷
- DeleteVolume：删除卷
- ControllerPublishVolume：附加卷
- ControllerUnpublishVolume：分离卷
- CreateSnapshot：创建快照
- DeleteSnapshot：删除快照

**CSI Node**：
- NodeStageVolume：挂载到临时目录
- NodeUnstageVolume：卸载
- NodePublishVolume：挂载到Pod目录
- NodeUnpublishVolume：卸载

**CSI Driver 示例：**
```yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: csi.example.com
spec:
  attachRequired: true
  podInfoOnMount: true
  volumeLifecycleModes:
  - Persistent
  - Ephemeral
```

### 2. 存储方案

**块存储（Block Storage）**：
- Ceph RBD
- AWS EBS
- Azure Disk
- GCE PD
- iSCSI

**特点：**
- 高性能
- 单节点访问（RWO）
- 适合数据库

**文件存储（File Storage）**：
- NFS
- CephFS
- AWS EFS
- Azure Files
- GlusterFS

**特点：**
- 多节点访问（RWX）
- POSIX兼容
- 适合共享数据

**对象存储（Object Storage）**：
- S3
- MinIO
- Ceph Object Gateway
- Azure Blob
- GCS

**特点：**
- 海量存储
- HTTP API
- 适合非结构化数据

### 3. 存储类详解

**StorageClass 参数：**
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  fsType: ext4
  encrypted: "true"
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
mountOptions:
- debug
- noatime
```

**回收策略：**
- Retain：保留数据，手动回收
- Delete：自动删除
- Recycle：清空数据（已废弃）

**绑定模式：**
- Immediate：立即绑定
- WaitForFirstConsumer：等待Pod调度

### 4. 快照和克隆

**VolumeSnapshot**：
```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: snapshot-1
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: pvc-1
```

**VolumeSnapshotClass**：
```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapclass
driver: csi.example.com
deletionPolicy: Delete
parameters:
  snapshot-type: incremental
```

**从快照恢复：**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restore-pvc
spec:
  dataSource:
    name: snapshot-1
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

**Volume Cloning**：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clone-pvc
spec:
  dataSource:
    name: source-pvc
    kind: PersistentVolumeClaim
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```


## 六、云原生网络

### 1. CNI（容器网络接口）

**作用：**
- 标准化网络接口
- 插件化架构
- 支持多种网络方案
- 网络配置管理

**CNI 插件类型：**
- Main插件：创建网络接口（bridge、ipvlan、macvlan）
- IPAM插件：IP地址管理（host-local、dhcp）
- Meta插件：附加功能（portmap、bandwidth、firewall）

**CNI 配置：**
```json
{
  "cniVersion": "0.4.0",
  "name": "mynet",
  "type": "bridge",
  "bridge": "cni0",
  "isGateway": true,
  "ipMasq": true,
  "ipam": {
    "type": "host-local",
    "subnet": "10.244.0.0/16",
    "routes": [
      { "dst": "0.0.0.0/0" }
    ]
  }
}
```

**常见实现：**
- Calico：BGP路由，网络策略
- Flannel：简单易用，Overlay网络
- Weave：自动发现，加密通信
- Cilium：eBPF，高性能，七层策略
- Canal：Flannel + Calico

### 2. Calico

**特点：**
- 纯三层网络
- BGP路由协议
- 网络策略支持
- 高性能
- 大规模集群

**网络模式：**

**IPIP 模式**：
- IP-in-IP封装
- 跨子网通信
- 性能损耗小

**BGP 模式**：
- 纯路由模式
- 无封装
- 最高性能
- 需要路由器支持

**VXLAN 模式**：
- 覆盖网络
- 兼容性好
- 性能中等

**配置：**
```yaml
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: default-ipv4-ippool
spec:
  cidr: 192.168.0.0/16
  ipipMode: CrossSubnet
  natOutgoing: true
  nodeSelector: all()
```

**网络策略：**
```yaml
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-tcp-6379
  namespace: production
spec:
  selector: role == 'database'
  types:
  - Ingress
  - Egress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: role == 'frontend'
    destination:
      ports:
      - 6379
  egress:
  - action: Allow
```

**全局网络策略：**
```yaml
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: deny-all
spec:
  selector: all()
  types:
  - Ingress
  - Egress
  egress:
  - action: Allow
    protocol: UDP
    destination:
      ports:
      - 53
```

### 3. Cilium

**特点：**
- 基于eBPF
- 内核级性能
- 七层网络策略
- 服务网格功能
- 可观测性

**eBPF 优势：**
- 无需内核模块
- 动态加载
- 高性能
- 安全隔离

**网络策略：**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: l7-rule
spec:
  endpointSelector:
    matchLabels:
      app: myapp
  ingress:
  - fromEndpoints:
    - matchLabels:
        app: frontend
    toPorts:
    - ports:
      - port: "80"
        protocol: TCP
      rules:
        http:
        - method: "GET"
          path: "/api/v1/.*"
```

**服务网格：**
```yaml
apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: envoy-lb
spec:
  services:
  - name: myapp
    namespace: default
  backendServices:
  - name: myapp-v1
    namespace: default
    number:
    - "80"
  - name: myapp-v2
    namespace: default
    number:
    - "80"
```

### 4. Service Mesh 网络

**Sidecar 模式：**
- 每个Pod一个代理
- 透明拦截流量
- 无侵入
- 资源消耗较高

**Ambient Mesh（无Sidecar）**：
- 节点级代理
- 降低资源消耗
- 简化运维
- Istio 1.15+

**流量拦截：**
```yaml
# iptables规则
-A PREROUTING -p tcp -j ISTIO_INBOUND
-A OUTPUT -p tcp -j ISTIO_OUTPUT

# Envoy监听
- 15001: Outbound
- 15006: Inbound
- 15020: Health check
- 15021: Health check
- 15090: Prometheus metrics
```

## 七、云原生安全

### 1. 镜像安全

**扫描工具：**

**Trivy**：
```bash
# 扫描镜像
trivy image nginx:latest

# 扫描文件系统
trivy fs /path/to/project

# 扫描配置文件
trivy config .

# 输出JSON
trivy image --format json nginx:latest

# 只显示高危漏洞
trivy image --severity HIGH,CRITICAL nginx:latest
```

**Clair**：
- CoreOS开源
- 静态分析
- 漏洞数据库

**Anchore**：
- 策略引擎
- 合规检查
- CI/CD集成

**最佳实践：**
- 使用官方镜像
- 定期更新基础镜像
- 最小化镜像
- 多阶段构建
- 签名验证
- 私有仓库

**镜像签名（Cosign）**：
```bash
# 生成密钥对
cosign generate-key-pair

# 签名镜像
cosign sign --key cosign.key myimage:v1

# 验证签名
cosign verify --key cosign.pub myimage:v1
```

### 2. 运行时安全

**Falco**：
```yaml
# 规则示例
- rule: Unauthorized Process
  desc: Detect unauthorized process in container
  condition: >
    spawned_process and
    container and
    not proc.name in (allowed_processes)
  output: >
    Unauthorized process started
    (user=%user.name command=%proc.cmdline container=%container.name)
  priority: WARNING

- rule: Write below root
  desc: Detect write below root directory
  condition: >
    write and
    container and
    fd.name startswith /
  output: >
    File write below root
    (user=%user.name file=%fd.name container=%container.name)
  priority: ERROR
```

**AppArmor**：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: apparmor-pod
  annotations:
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-nginx
spec:
  containers:
  - name: nginx
    image: nginx
```

**SELinux**：
```yaml
spec:
  securityContext:
    seLinuxOptions:
      level: "s0:c123,c456"
      type: "container_t"
```

**Seccomp**：
```yaml
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault
      # type: Localhost
      # localhostProfile: profiles/audit.json
```

### 3. 准入控制

**OPA（Open Policy Agent）**：
```rego
package kubernetes.admission

deny[msg] {
  input.request.kind.kind == "Pod"
  image := input.request.object.spec.containers[_].image
  not startswith(image, "myregistry.com/")
  msg := sprintf("Image '%v' not from approved registry", [image])
}

deny[msg] {
  input.request.kind.kind == "Pod"
  not input.request.object.spec.securityContext.runAsNonRoot
  msg := "Containers must not run as root"
}
```

**Gatekeeper**：
```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
  - target: admission.k8s.gatekeeper.sh
    rego: |
      package k8srequiredlabels
      
      violation[{"msg": msg}] {
        provided := {label | input.review.object.metadata.labels[label]}
        required := {label | label := input.parameters.labels[_]}
        missing := required - provided
        count(missing) > 0
        msg := sprintf("Missing required labels: %v", [missing])
      }
```

**Pod Security Standards**：
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

**级别：**
- Privileged：无限制
- Baseline：最小限制
- Restricted：严格限制

### 4. 密钥管理

**Vault**：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: vault-agent
  annotations:
    vault.hashicorp.com/agent-inject: "true"
    vault.hashicorp.com/role: "myapp"
    vault.hashicorp.com/agent-inject-secret-database: "database/creds/myapp"
spec:
  serviceAccountName: myapp
  containers:
  - name: app
    image: myapp:latest
```

**Sealed Secrets**：
```bash
# 创建sealed secret
kubectl create secret generic mysecret \
  --from-literal=password=mypassword \
  --dry-run=client -o yaml | \
  kubeseal -o yaml > mysealedsecret.yaml

# 应用
kubectl apply -f mysealedsecret.yaml
```

**External Secrets Operator**：
```yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: example
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: secret-to-be-created
  data:
  - secretKey: password
    remoteRef:
      key: secret/data/myapp
      property: password
```

## 八、CI/CD

### 1. GitOps

**原则：**
- Git作为唯一真实来源
- 声明式配置
- 自动化部署
- 持续同步
- 版本控制

**优势：**
- 可审计
- 易回滚
- 团队协作
- 灾难恢复

**工具：**
- ArgoCD
- Flux
- Jenkins X
- Tekton

### 2. ArgoCD

**安装：**
```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

**Application 定义：**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/myapp
    targetRevision: HEAD
    path: k8s
    helm:
      valueFiles:
      - values-prod.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

**多集群管理：**
```bash
# 添加集群
argocd cluster add my-cluster

# 查看集群
argocd cluster list

# Application指定集群
spec:
  destination:
    name: my-cluster
    namespace: production
```

**ApplicationSet**：
```yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cluster-apps
spec:
  generators:
  - list:
      elements:
      - cluster: dev
        url: https://dev.example.com
      - cluster: prod
        url: https://prod.example.com
  template:
    metadata:
      name: '{{cluster}}-myapp'
    spec:
      source:
        repoURL: https://github.com/myorg/myapp
        path: k8s
      destination:
        server: '{{url}}'
        namespace: myapp
```

### 3. Tekton

**Task 定义：**
```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: build-image
spec:
  params:
  - name: image
    type: string
  - name: dockerfile
    type: string
    default: Dockerfile
  workspaces:
  - name: source
  steps:
  - name: build
    image: gcr.io/kaniko-project/executor:latest
    args:
    - --dockerfile=$(params.dockerfile)
    - --context=$(workspaces.source.path)
    - --destination=$(params.image)
```

**Pipeline 定义：**
```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: build-and-deploy
spec:
  params:
  - name: repo-url
  - name: image-name
  workspaces:
  - name: shared-workspace
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    params:
    - name: url
      value: $(params.repo-url)
    workspaces:
    - name: output
      workspace: shared-workspace
  
  - name: build-image
    taskRef:
      name: build-image
    runAfter:
    - fetch-source
    params:
    - name: image
      value: $(params.image-name)
    workspaces:
    - name: source
      workspace: shared-workspace
  
  - name: deploy
    taskRef:
      name: kubectl-deploy
    runAfter:
    - build-image
    params:
    - name: image
      value: $(params.image-name)
```

**PipelineRun 执行：**
```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: build-and-deploy-run
spec:
  pipelineRef:
    name: build-and-deploy
  params:
  - name: repo-url
    value: https://github.com/myorg/myapp
  - name: image-name
    value: myregistry.com/myapp:v1.0.0
  workspaces:
  - name: shared-workspace
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
```

### 4. 镜像构建

**Kaniko**：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kaniko
spec:
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:latest
    args:
    - --dockerfile=Dockerfile
    - --context=git://github.com/myorg/myapp
    - --destination=myregistry.com/myapp:v1.0.0
    volumeMounts:
    - name: docker-config
      mountPath: /kaniko/.docker
  volumes:
  - name: docker-config
    secret:
      secretName: regcred
      items:
      - key: .dockerconfigjson
        path: config.json
```

**Buildah**：
```bash
# 构建镜像
buildah bud -t myapp:v1 .

# 推送镜像
buildah push myapp:v1 docker://myregistry.com/myapp:v1

# 在容器中运行
buildah from alpine
buildah run alpine-working-container apk add nginx
buildah commit alpine-working-container mynginx
```

**BuildKit**：
```bash
# 启用BuildKit
export DOCKER_BUILDKIT=1

# 构建
docker build -t myapp:v1 .

# 多平台构建
docker buildx build --platform linux/amd64,linux/arm64 -t myapp:v1 .
```

**img**：
```bash
# 无守护进程构建
img build -t myapp:v1 .

# 推送
img push myapp:v1
```

## 九、可观测性

### 1. 监控（Metrics）

**Prometheus**：
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
```

**指标类型：**

**Counter（计数器）**：
```go
httpRequestsTotal := prometheus.NewCounter(
    prometheus.CounterOpts{
        Name: "http_requests_total",
        Help: "Total number of HTTP requests",
    },
)
httpRequestsTotal.Inc()
```

**Gauge（仪表盘）**：
```go
cpuUsage := prometheus.NewGauge(
    prometheus.GaugeOpts{
        Name: "cpu_usage_percent",
        Help: "Current CPU usage",
    },
)
cpuUsage.Set(75.5)
```

**Histogram（直方图）**：
```go
httpDuration := prometheus.NewHistogram(
    prometheus.HistogramOpts{
        Name: "http_request_duration_seconds",
        Help: "HTTP request duration",
        Buckets: prometheus.DefBuckets,
    },
)
httpDuration.Observe(0.42)
```

**Summary（摘要）**：
```go
httpDuration := prometheus.NewSummary(
    prometheus.SummaryOpts{
        Name: "http_request_duration_seconds",
        Help: "HTTP request duration",
        Objectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},
    },
)
```

**PromQL 查询：**
```promql
# 请求速率
rate(http_requests_total[5m])

# P95延迟
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# 错误率
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# CPU使用率
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# 内存使用率
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
```

**Grafana 仪表板：**
```json
{
  "dashboard": {
    "title": "Application Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])"
          }
        ]
      }
    ]
  }
}
```

### 2. 日志（Logging）

**ELK Stack**：

**Filebeat 配置：**
```yaml
filebeat.inputs:
- type: container
  paths:
    - /var/log/containers/*.log
  processors:
  - add_kubernetes_metadata:
      host: ${NODE_NAME}
      matchers:
      - logs_path:
          logs_path: "/var/log/containers/"

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "k8s-logs-%{+yyyy.MM.dd}"
```

**Logstash 配置：**
```ruby
input {
  beats {
    port => 5044
  }
}

filter {
  json {
    source => "message"
  }
  
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
  
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
```

**Loki**：
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
    
    positions:
      filename: /tmp/positions.yaml
    
    clients:
    - url: http://loki:3100/loki/api/v1/push
    
    scrape_configs:
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
```

**LogQL 查询：**
```logql
# 查询特定应用日志
{app="myapp"}

# 过滤错误日志
{app="myapp"} |= "error"

# 正则匹配
{app="myapp"} |~ "error|ERROR|Error"

# 统计错误数
sum(rate({app="myapp"} |= "error" [5m]))

# 提取字段
{app="myapp"} | json | status="500"
```

### 3. 追踪（Tracing）

**Jaeger**：
```yaml
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
  ingress:
    enabled: true
```

**OpenTelemetry**：
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    
    processors:
      batch:
      memory_limiter:
        limit_mib: 512
        spike_limit_mib: 128
    
    exporters:
      jaeger:
        endpoint: jaeger:14250
        tls:
          insecure: true
      prometheus:
        endpoint: "0.0.0.0:8889"
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [jaeger]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheus]
```

**应用埋点（Go）**：
```go
import (
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/trace"
)

func handleRequest(ctx context.Context) {
    tracer := otel.Tracer("myapp")
    ctx, span := tracer.Start(ctx, "handleRequest")
    defer span.End()
    
    // 添加属性
    span.SetAttributes(
        attribute.String("user.id", "123"),
        attribute.Int("http.status_code", 200),
    )
    
    // 添加事件
    span.AddEvent("Processing request")
    
    // 调用其他服务
    callDatabase(ctx)
}
```


### 4. 告警（Alerting）

**Alertmanager 配置：**
```yaml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'pagerduty'
  - match:
      severity: warning
    receiver: 'slack'

receivers:
- name: 'default'
  email_configs:
  - to: 'team@example.com'

- name: 'slack'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/xxx'
    channel: '#alerts'
    title: '{{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

- name: 'pagerduty'
  pagerduty_configs:
  - service_key: 'xxx'

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
```

**告警规则：**
```yaml
groups:
- name: example
  interval: 30s
  rules:
  # 高CPU使用率
  - alert: HighCPUUsage
    expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is {{ $value }}%"
  
  # 高内存使用率
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is {{ $value }}%"
  
  # Pod重启频繁
  - alert: PodRestartingTooOften
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} restarting too often"
      description: "Pod has restarted {{ $value }} times in the last 15 minutes"
  
  # 服务不可用
  - alert: ServiceDown
    expr: up{job="myapp"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Service {{ $labels.job }} is down"
      description: "{{ $labels.instance }} has been down for more than 1 minute"
  
  # 高错误率
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate on {{ $labels.job }}"
      description: "Error rate is {{ $value | humanizePercentage }}"
  
  # 高延迟
  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High latency on {{ $labels.job }}"
      description: "P95 latency is {{ $value }}s"
```

## 十、云原生数据库

### 1. Operator 模式

**Operator 框架：**
- Kubebuilder
- Operator SDK
- KUDO

**自定义资源（CRD）**：
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: mysqls.database.example.com
spec:
  group: database.example.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              version:
                type: string
              replicas:
                type: integer
              storage:
                type: string
  scope: Namespaced
  names:
    plural: mysqls
    singular: mysql
    kind: MySQL
    shortNames:
    - my
```

**MySQL Operator 示例：**
```yaml
apiVersion: database.example.com/v1
kind: MySQL
metadata:
  name: mysql-cluster
spec:
  version: "8.0"
  replicas: 3
  storage: 10Gi
  storageClassName: fast
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1"
  backup:
    enabled: true
    schedule: "0 2 * * *"
    retention: 7
```

### 2. StatefulSet 应用

**MySQL 主从复制：**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:8.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
      
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          if [[ -f xtrabackup_slave_info ]]; then
            mv xtrabackup_slave_info change_master_to.sql.in
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi
          
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
            
            echo "Initializing replication from clone position"
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql-config
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast
      resources:
        requests:
          storage: 10Gi
```

**Redis 集群：**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  serviceName: redis
  replicas: 6
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.0-alpine
        command:
        - redis-server
        - /conf/redis.conf
        - --cluster-enabled
        - "yes"
        - --cluster-config-file
        - /data/nodes.conf
        - --cluster-node-timeout
        - "5000"
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        volumeMounts:
        - name: conf
          mountPath: /conf
        - name: data
          mountPath: /data
      volumes:
      - name: conf
        configMap:
          name: redis-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
```

### 3. 备份恢复

**Velero**：
```bash
# 安装
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.5.0 \
  --bucket velero-backups \
  --secret-file ./credentials-velero

# 备份整个集群
velero backup create full-backup

# 备份特定命名空间
velero backup create app-backup --include-namespaces app

# 定时备份
velero schedule create daily-backup --schedule="0 2 * * *"

# 恢复
velero restore create --from-backup full-backup

# 查看备份
velero backup get

# 查看恢复
velero restore get
```

**备份策略：**
```yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: app-backup
spec:
  includedNamespaces:
  - app
  includedResources:
  - deployments
  - services
  - configmaps
  - secrets
  - persistentvolumeclaims
  excludedResources:
  - events
  labelSelector:
    matchLabels:
      backup: "true"
  ttl: 720h
  storageLocation: default
  volumeSnapshotLocations:
  - default
```

## 十一、多集群管理

### 1. 集群联邦（Federation）

**KubeFed**：
```yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: test-deployment
  namespace: test
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:latest
  placement:
    clusters:
    - name: cluster1
    - name: cluster2
  overrides:
  - clusterName: cluster1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
  - clusterName: cluster2
    clusterOverrides:
    - path: "/spec/replicas"
      value: 3
```

### 2. 多租户

**命名空间隔离：**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
  labels:
    tenant: a
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-quota
  namespace: tenant-a
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    pods: "50"
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-from-other-namespaces
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
```

**vCluster（虚拟集群）**：
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: vcluster-tenant-a
---
apiVersion: v1
kind: Service
metadata:
  name: vcluster
  namespace: vcluster-tenant-a
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 8443
  selector:
    app: vcluster
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vcluster
  namespace: vcluster-tenant-a
spec:
  serviceName: vcluster
  replicas: 1
  selector:
    matchLabels:
      app: vcluster
  template:
    metadata:
      labels:
        app: vcluster
    spec:
      containers:
      - name: vcluster
        image: rancher/k3s:v1.25.0-k3s1
        command:
        - /bin/k3s
        - server
        - --write-kubeconfig=/data/k3s-config/kube-config.yaml
        - --data-dir=/data
        - --disable=traefik,servicelb,metrics-server,local-storage
        - --disable-network-policy
        - --disable-agent
        - --disable-scheduler
        - --disable-cloud-controller
        - --flannel-backend=none
        - --kube-controller-manager-arg=controllers=*,-nodeipam,-nodelifecycle,-persistentvolume-binder,-attachdetach,-persistentvolume-expander,-cloud-node-lifecycle
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
```

## 十二、Serverless

### 1. Knative

**Knative Serving**：
```yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: hello
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "0"
        autoscaling.knative.dev/maxScale: "10"
        autoscaling.knative.dev/target: "100"
    spec:
      containers:
      - image: gcr.io/knative-samples/helloworld-go
        env:
        - name: TARGET
          value: "World"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
```

**流量分割：**
```yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: hello
spec:
  traffic:
  - revisionName: hello-v1
    percent: 90
  - revisionName: hello-v2
    percent: 10
    tag: canary
```

**Knative Eventing**：
```yaml
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  name: default
---
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: my-trigger
spec:
  broker: default
  filter:
    attributes:
      type: dev.knative.samples.helloworld
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: hello
```

### 2. OpenFaaS

**函数定义：**
```yaml
apiVersion: openfaas.com/v1
kind: Function
metadata:
  name: hello-python
spec:
  name: hello-python
  image: hello-python:latest
  labels:
    com.openfaas.scale.min: "1"
    com.openfaas.scale.max: "10"
  environment:
    write_timeout: 10s
    read_timeout: 10s
  limits:
    memory: "128Mi"
    cpu: "100m"
  requests:
    memory: "64Mi"
    cpu: "50m"
```

## 十三、边缘计算

### 1. KubeEdge

**架构：**
- CloudCore：云端组件
- EdgeCore：边缘节点组件
- DeviceTwin：设备管理

**部署边缘节点：**
```bash
# 云端安装
keadm init --advertise-address=<cloud-ip>

# 边缘节点加入
keadm join --cloudcore-ipport=<cloud-ip>:10000 --token=<token>
```

**边缘应用：**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edge-app
  template:
    metadata:
      labels:
        app: edge-app
    spec:
      nodeSelector:
        node-role.kubernetes.io/edge: ""
      containers:
      - name: app
        image: myapp:latest
```

### 2. K3s

**特点：**
- 单二进制文件（<100MB）
- 低资源消耗
- 适合边缘和IoT
- 完整K8s功能

**安装：**
```bash
# Server节点
curl -sfL https://get.k3s.io | sh -

# Agent节点
curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -
```

## 十四、成本优化

### 1. 资源优化

**VPA 推荐：**
```bash
# 查看VPA推荐
kubectl describe vpa myapp-vpa

# 推荐示例
Recommendation:
  Container Recommendations:
    Container Name:  app
    Lower Bound:
      Cpu:     100m
      Memory:  128Mi
    Target:
      Cpu:     250m
      Memory:  256Mi
    Upper Bound:
      Cpu:     500m
      Memory:  512Mi
```

**Spot 实例：**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: spot-pod
spec:
  nodeSelector:
    kubernetes.io/lifecycle: spot
  tolerations:
  - key: "spot"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: app
    image: myapp:latest
```

### 2. 集群优化

**Cluster Autoscaler**：
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.25.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
```

### 3. 成本监控

**Kubecost**：
```bash
# 安装
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost --create-namespace

# 访问
kubectl port-forward -n kubecost svc/kubecost-cost-analyzer 9090:9090
```

## 十五、最佳实践

### 1. 12因素应用

1. **代码库**：一份代码，多份部署
2. **依赖**：显式声明依赖
3. **配置**：环境变量存储配置
4. **后端服务**：附加资源
5. **构建、发布、运行**：严格分离
6. **进程**：无状态进程
7. **端口绑定**：自包含服务
8. **并发**：进程模型扩展
9. **易处理**：快速启动和优雅终止
10. **开发环境与生产环境等价**
11. **日志**：事件流
12. **管理进程**：一次性任务

### 2. 高可用设计

**多副本部署：**
```yaml
spec:
  replicas: 3
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: myapp
        topologyKey: kubernetes.io/hostname
```

**跨可用区：**
```yaml
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: myapp
          topologyKey: topology.kubernetes.io/zone
```

**优雅终止：**
```yaml
spec:
  terminationGracePeriodSeconds: 30
  containers:
  - name: app
    lifecycle:
      preStop:
        exec:
          command: ["/bin/sh", "-c", "sleep 15"]
```

### 3. 安全加固

**Pod Security Context**：
```yaml
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
```

### 4. 性能优化

**DNS 缓存：**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        cache 30
        reload
        loadbalance
    }
```

**连接池：**
```go
db, err := sql.Open("mysql", dsn)
db.SetMaxOpenConns(25)
db.SetMaxIdleConns(25)
db.SetConnMaxLifetime(5 * time.Minute)
```

---

**本文档持续更新中，涵盖容器与云原生技术栈的核心知识点。**

**相关资源：**
- Docker官方文档：https://docs.docker.com
- Kubernetes官方文档：https://kubernetes.io/docs
- CNCF项目：https://www.cncf.io/projects
- 云原生技术社区：https://cloudnative.to
